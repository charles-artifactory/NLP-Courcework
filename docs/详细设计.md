# RAGå¢å¼ºæ™ºèƒ½é—®ç­”ç³»ç»Ÿ - è¯¦ç»†è®¾è®¡æ–‡æ¡£

## 1. å¼•è¨€

### 1.1 æ–‡æ¡£ç›®çš„

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°RAGå¢å¼ºæ™ºèƒ½é—®ç­”ç³»ç»Ÿå„æ¨¡å—çš„å†…éƒ¨è®¾è®¡ï¼ŒåŒ…æ‹¬ç±»è®¾è®¡ã€æ¥å£å®šä¹‰ã€ç®—æ³•è¯¦è§£å’Œæ•°æ®ç»“æ„ï¼Œä¸ºç¼–ç å®ç°æä¾›ç›´æ¥æŒ‡å¯¼ã€‚

### 1.2 å‚è€ƒæ–‡æ¡£

- ã€Šéœ€æ±‚åˆ†æ.mdã€‹
- ã€Šæ¦‚è¦è®¾è®¡.mdã€‹

---

## 2. æ¨¡å—è¯¦ç»†è®¾è®¡

### 2.1 é…ç½®ç®¡ç†æ¨¡å— (config.py)

#### 2.1.1 ç±»è®¾è®¡

```python
class Config:
    """ç³»ç»Ÿé…ç½®ç®¡ç†ç±»"""
    
    # æ–‡æ¡£å¤„ç†é…ç½®
    CHUNK_SIZE: int = 512          # åˆ†å—å¤§å°(tokens)
    CHUNK_OVERLAP: int = 64        # åˆ†å—é‡å 
    MAX_FILE_SIZE: int = 50 * 1024 * 1024  # æœ€å¤§æ–‡ä»¶å¤§å°(50MB)
    SUPPORTED_FORMATS: List[str] = [".pdf", ".txt", ".docx", ".md"]
    
    # åµŒå…¥æ¨¡å‹é…ç½®
    EMBEDDING_MODEL: str = "BAAI/bge-m3"
    EMBEDDING_DIM: int = 1024
    EMBEDDING_BATCH_SIZE: int = 32
    
    # å‘é‡æ•°æ®åº“é…ç½®
    VECTOR_DB_PATH: str = "./data/chroma_db"
    COLLECTION_NAME: str = "documents"
    
    # æ£€ç´¢é…ç½®
    TOP_K: int = 5
    RERANK_TOP_K: int = 10
    HYBRID_ALPHA: float = 0.7      # æ··åˆæ£€ç´¢æƒé‡
    
    # ç”Ÿæˆé…ç½®
    LLM_MODEL: str = "qwen2.5:7b"
    MAX_NEW_TOKENS: int = 1024
    TEMPERATURE: float = 0.7
    
    # æœåŠ¡é…ç½®
    HOST: str = "0.0.0.0"
    PORT: int = 7860
```

---

### 2.2 æ–‡æ¡£å¤„ç†æ¨¡å— (document_processor.py)

#### 2.2.1 ç±»å›¾

```mermaid
classDiagram
    class DocumentProcessor {
        -config: Config
        -chunker: SemanticChunker
        +load_document(file_path: str) Document
        +parse_pdf(file_path: str) str
        +parse_docx(file_path: str) str
        +parse_txt(file_path: str) str
        +parse_markdown(file_path: str) str
        +process(file_path: str) List~Chunk~
    }
    
    class SemanticChunker {
        -chunk_size: int
        -chunk_overlap: int
        -embedder: Embedder
        +chunk(text: str) List~Chunk~
        -split_sentences(text: str) List~str~
        -compute_breakpoints(sentences: List~str~) List~int~
        -merge_chunks(sentences: List~str~, breakpoints: List~int~) List~Chunk~
    }
    
    class Document {
        +id: str
        +filename: str
        +content: str
        +format: str
        +metadata: Dict
        +created_at: datetime
    }
    
    class Chunk {
        +id: str
        +document_id: str
        +content: str
        +start_pos: int
        +end_pos: int
        +metadata: Dict
    }
    
    DocumentProcessor --> SemanticChunker
    DocumentProcessor --> Document
    SemanticChunker --> Chunk
```

#### 2.2.2 æ ¸å¿ƒæ–¹æ³•è¯¦è§£

##### load_document()

```python
def load_document(self, file_path: str) -> Document:
    """
    åŠ è½½å¹¶è§£ææ–‡æ¡£
    
    Args:
        file_path: æ–‡æ¡£è·¯å¾„
        
    Returns:
        Document: è§£æåçš„æ–‡æ¡£å¯¹è±¡
        
    Raises:
        ValueError: ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
        FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨
    """
```

**å¤„ç†æµç¨‹**ï¼š
1. éªŒè¯æ–‡ä»¶å­˜åœ¨æ€§å’Œæ ¼å¼
2. æ ¹æ®æ‰©å±•åé€‰æ‹©è§£æå™¨
3. æå–æ–‡æœ¬å†…å®¹å’Œå…ƒæ•°æ®
4. æ„å»ºDocumentå¯¹è±¡è¿”å›

##### SemanticChunker.chunk() - åˆ›æ–°ç®—æ³•

```python
def chunk(self, text: str, doc_id: str) -> List[Chunk]:
    """
    åŸºäºè¯­ä¹‰è¾¹ç•Œçš„æ™ºèƒ½åˆ†å—
    
    ç®—æ³•æ­¥éª¤ï¼š
    1. ä½¿ç”¨å¥å­åˆ†å‰²å™¨åˆ‡åˆ†æ–‡æœ¬
    2. è®¡ç®—ç›¸é‚»å¥å­çš„åµŒå…¥å‘é‡
    3. è®¡ç®—ç›¸é‚»å¥å­é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
    4. æ‰¾å‡ºç›¸ä¼¼åº¦ä½äºé˜ˆå€¼çš„ä½ç½®ä½œä¸ºæ½œåœ¨åˆ†å‰²ç‚¹
    5. æ ¹æ®ç›®æ ‡å—å¤§å°åˆå¹¶å¥å­ï¼Œä¼˜å…ˆåœ¨æ½œåœ¨åˆ†å‰²ç‚¹åˆ‡åˆ†
    """
```

**ç®—æ³•ä¼ªä»£ç **ï¼š
```
function semantic_chunk(text, target_size, overlap):
    sentences = split_into_sentences(text)
    embeddings = embed_sentences(sentences)
    
    # è®¡ç®—ç›¸é‚»å¥å­ç›¸ä¼¼åº¦
    similarities = []
    for i in range(len(sentences) - 1):
        sim = cosine_similarity(embeddings[i], embeddings[i+1])
        similarities.append(sim)
    
    # æ‰¾å‡ºè¯­ä¹‰æ–­ç‚¹ï¼ˆç›¸ä¼¼åº¦ä½çš„ä½ç½®ï¼‰
    threshold = percentile(similarities, 25)
    breakpoints = [i for i, s in enumerate(similarities) if s < threshold]
    
    # åˆå¹¶æˆå—
    chunks = []
    current_chunk = []
    current_size = 0
    
    for i, sentence in enumerate(sentences):
        if current_size + len(sentence) > target_size and i in breakpoints:
            chunks.append(join(current_chunk))
            # ä¿ç•™overlap
            current_chunk = current_chunk[-overlap:]
            current_size = sum(len(s) for s in current_chunk)
        current_chunk.append(sentence)
        current_size += len(sentence)
    
    if current_chunk:
        chunks.append(join(current_chunk))
    
    return chunks
```

---

### 2.3 åµŒå…¥æ¨¡å— (embedder.py)

#### 2.3.1 ç±»å›¾

```mermaid
classDiagram
    class Embedder {
        -model: SentenceTransformer
        -config: Config
        +__init__(model_name: str)
        +embed_text(text: str) ndarray
        +embed_batch(texts: List~str~) ndarray
        +embed_query(query: str) ndarray
    }
    
    class VectorStore {
        -client: ChromaClient
        -collection: Collection
        -embedder: Embedder
        +__init__(path: str, collection_name: str)
        +add_chunks(chunks: List~Chunk~) None
        +search(query_embedding: ndarray, top_k: int) List~SearchResult~
        +delete_document(doc_id: str) None
        +get_all_documents() List~Document~
    }
    
    class SearchResult {
        +chunk_id: str
        +content: str
        +score: float
        +metadata: Dict
    }
    
    Embedder --> VectorStore
    VectorStore --> SearchResult
```

#### 2.3.2 æ ¸å¿ƒæ–¹æ³•è¯¦è§£

##### embed_batch()

```python
def embed_batch(self, texts: List[str]) -> np.ndarray:
    """
    æ‰¹é‡æ–‡æœ¬åµŒå…¥
    
    Args:
        texts: æ–‡æœ¬åˆ—è¡¨
        
    Returns:
        embeddings: å½¢çŠ¶ä¸º (n, dim) çš„åµŒå…¥çŸ©é˜µ
        
    å®ç°ç»†èŠ‚ï¼š
    - ä½¿ç”¨æ¨¡å‹çš„encodeæ–¹æ³•
    - è‡ªåŠ¨æ‰¹å¤„ç†ä¼˜åŒ–å†…å­˜
    - å½’ä¸€åŒ–å‘é‡ä¾¿äºä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
    """
    embeddings = self.model.encode(
        texts,
        batch_size=self.config.EMBEDDING_BATCH_SIZE,
        normalize_embeddings=True,
        show_progress_bar=True
    )
    return embeddings
```

##### VectorStore.add_chunks()

```python
def add_chunks(self, chunks: List[Chunk]) -> None:
    """
    æ·»åŠ æ–‡æœ¬å—åˆ°å‘é‡æ•°æ®åº“
    
    Args:
        chunks: æ–‡æœ¬å—åˆ—è¡¨
        
    å®ç°æµç¨‹ï¼š
    1. æå–æ–‡æœ¬å†…å®¹åˆ—è¡¨
    2. æ‰¹é‡è®¡ç®—åµŒå…¥å‘é‡
    3. æ„å»ºå…ƒæ•°æ®
    4. è°ƒç”¨ChromaDBçš„addæ–¹æ³•
    """
    texts = [chunk.content for chunk in chunks]
    embeddings = self.embedder.embed_batch(texts)
    
    ids = [chunk.id for chunk in chunks]
    metadatas = [
        {
            "document_id": chunk.document_id,
            "start_pos": chunk.start_pos,
            "end_pos": chunk.end_pos,
            **chunk.metadata
        }
        for chunk in chunks
    ]
    
    self.collection.add(
        ids=ids,
        embeddings=embeddings.tolist(),
        documents=texts,
        metadatas=metadatas
    )
```

---

### 2.4 æ£€ç´¢æ¨¡å— (retriever.py)

#### 2.4.1 ç±»å›¾

```mermaid
classDiagram
    class BaseRetriever {
        <<abstract>>
        +retrieve(query: str, top_k: int) List~SearchResult~
    }
    
    class DenseRetriever {
        -embedder: Embedder
        -vector_store: VectorStore
        +retrieve(query: str, top_k: int) List~SearchResult~
    }
    
    class SparseRetriever {
        -bm25: BM25Okapi
        -documents: List~str~
        -chunk_ids: List~str~
        +build_index(chunks: List~Chunk~) None
        +retrieve(query: str, top_k: int) List~SearchResult~
    }
    
    class HybridRetriever {
        -dense_retriever: DenseRetriever
        -sparse_retriever: SparseRetriever
        -alpha: float
        +retrieve(query: str, top_k: int) List~SearchResult~
        -normalize_scores(results: List~SearchResult~) List~SearchResult~
        -fuse_results(dense: List, sparse: List) List~SearchResult~
    }
    
    class Reranker {
        -model: CrossEncoder
        +rerank(query: str, results: List~SearchResult~, top_k: int) List~SearchResult~
    }
    
    BaseRetriever <|-- DenseRetriever
    BaseRetriever <|-- SparseRetriever
    BaseRetriever <|-- HybridRetriever
    HybridRetriever --> DenseRetriever
    HybridRetriever --> SparseRetriever
    HybridRetriever --> Reranker
```

#### 2.4.2 æ··åˆæ£€ç´¢ç®—æ³• - åˆ›æ–°ç‚¹

```python
def retrieve(self, query: str, top_k: int = 5) -> List[SearchResult]:
    """
    æ··åˆæ£€ç´¢ï¼šç»“åˆç¨ å¯†æ£€ç´¢å’Œç¨€ç–æ£€ç´¢
    
    ç®—æ³•ï¼š
    1. ç¨ å¯†æ£€ç´¢è·å– top_k * 2 ç»“æœ
    2. ç¨€ç–æ£€ç´¢è·å– top_k * 2 ç»“æœ
    3. åˆ†æ•°å½’ä¸€åŒ–
    4. åŠ æƒèåˆ
    5. é‡æ’åºè·å–æœ€ç»ˆ top_k
    """
    # ç¨ å¯†æ£€ç´¢
    dense_results = self.dense_retriever.retrieve(query, top_k * 2)
    
    # ç¨€ç–æ£€ç´¢
    sparse_results = self.sparse_retriever.retrieve(query, top_k * 2)
    
    # å½’ä¸€åŒ–åˆ†æ•°åˆ° [0, 1]
    dense_results = self.normalize_scores(dense_results)
    sparse_results = self.normalize_scores(sparse_results)
    
    # åŠ æƒèåˆ
    fused_results = self.fuse_results(dense_results, sparse_results)
    
    # é‡æ’åº
    if self.reranker:
        fused_results = self.reranker.rerank(query, fused_results, top_k)
    
    return fused_results[:top_k]


def fuse_results(
    self, 
    dense: List[SearchResult], 
    sparse: List[SearchResult]
) -> List[SearchResult]:
    """
    èåˆç¨ å¯†å’Œç¨€ç–æ£€ç´¢ç»“æœ
    
    å…¬å¼: final_score = Î± Ã— dense_score + (1-Î±) Ã— sparse_score
    """
    score_map = {}
    result_map = {}
    
    # æ”¶é›†ç¨ å¯†æ£€ç´¢åˆ†æ•°
    for r in dense:
        score_map[r.chunk_id] = {"dense": r.score, "sparse": 0}
        result_map[r.chunk_id] = r
    
    # æ”¶é›†ç¨€ç–æ£€ç´¢åˆ†æ•°
    for r in sparse:
        if r.chunk_id in score_map:
            score_map[r.chunk_id]["sparse"] = r.score
        else:
            score_map[r.chunk_id] = {"dense": 0, "sparse": r.score}
            result_map[r.chunk_id] = r
    
    # è®¡ç®—èåˆåˆ†æ•°
    fused = []
    for chunk_id, scores in score_map.items():
        final_score = (
            self.alpha * scores["dense"] + 
            (1 - self.alpha) * scores["sparse"]
        )
        result = result_map[chunk_id]
        result.score = final_score
        fused.append(result)
    
    # æŒ‰åˆ†æ•°é™åºæ’åº
    fused.sort(key=lambda x: x.score, reverse=True)
    return fused
```

#### 2.4.3 é‡æ’åºç®—æ³• - åˆ›æ–°ç‚¹

```python
class Reranker:
    """ä½¿ç”¨Cross-Encoderè¿›è¡Œé‡æ’åº"""
    
    def __init__(self, model_name: str = "BAAI/bge-reranker-base"):
        self.model = CrossEncoder(model_name)
    
    def rerank(
        self, 
        query: str, 
        results: List[SearchResult], 
        top_k: int
    ) -> List[SearchResult]:
        """
        å¯¹æ£€ç´¢ç»“æœé‡æ’åº
        
        åŸç†ï¼š
        Cross-EncoderåŒæ—¶æ¥æ”¶queryå’Œdocumentä½œä¸ºè¾“å…¥ï¼Œ
        èƒ½å¤Ÿå»ºæ¨¡æ›´ç²¾ç»†çš„äº¤äº’å…³ç³»ï¼Œæ’åºæ•ˆæœä¼˜äºBi-Encoder
        """
        if not results:
            return results
        
        # æ„å»ºè¾“å…¥å¯¹
        pairs = [(query, r.content) for r in results]
        
        # è·å–é‡æ’åºåˆ†æ•°
        scores = self.model.predict(pairs)
        
        # æ›´æ–°åˆ†æ•°å¹¶æ’åº
        for i, result in enumerate(results):
            result.rerank_score = float(scores[i])
        
        results.sort(key=lambda x: x.rerank_score, reverse=True)
        return results[:top_k]
```

---

### 2.5 ç”Ÿæˆæ¨¡å— (generator.py)

#### 2.5.1 ç±»å›¾

```mermaid
classDiagram
    class Generator {
        -llm: OllamaLLM
        -prompt_builder: PromptBuilder
        -source_tracer: SourceTracer
        +generate(query: str, contexts: List~str~, history: List) str
        +generate_stream(query: str, contexts: List~str~, history: List) Iterator
    }
    
    class PromptBuilder {
        -system_prompt: str
        -rag_template: str
        +build_prompt(query: str, contexts: List~str~, history: List) str
        +build_messages(query: str, contexts: List~str~, history: List) List~Dict~
    }
    
    class SourceTracer {
        +trace_sources(answer: str, contexts: List~SearchResult~) List~SourceRef~
        +highlight_sources(answer: str, sources: List~SourceRef~) str
    }
    
    class SourceRef {
        +text: str
        +chunk_id: str
        +document: str
        +position: Tuple~int, int~
    }
    
    Generator --> PromptBuilder
    Generator --> SourceTracer
    SourceTracer --> SourceRef
```

#### 2.5.2 Promptæ¨¡æ¿è®¾è®¡

```python
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„å‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è§„åˆ™ï¼š
1. åªæ ¹æ®å‚è€ƒèµ„æ–™å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. å¦‚æœå‚è€ƒèµ„æ–™ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•å›ç­”"
3. å›ç­”æ—¶å¼•ç”¨æ¥æºï¼Œä½¿ç”¨[1], [2]ç­‰æ ‡è®°
4. å›ç­”è¦å‡†ç¡®ã€ç®€æ´ã€æœ‰æ¡ç†
5. ä½¿ç”¨ä¸ç”¨æˆ·é—®é¢˜ç›¸åŒçš„è¯­è¨€å›ç­”
"""

RAG_TEMPLATE = """å‚è€ƒèµ„æ–™ï¼š
{contexts}

å¯¹è¯å†å²ï¼š
{history}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·æ ¹æ®å‚è€ƒèµ„æ–™å›ç­”ä¸Šè¿°é—®é¢˜ï¼š"""
```

#### 2.5.3 æ¥æºè¿½è¸ªç®—æ³• - åˆ›æ–°ç‚¹

```python
class SourceTracer:
    """ç­”æ¡ˆæ¥æºè¿½è¸ªå™¨"""
    
    def trace_sources(
        self, 
        answer: str, 
        contexts: List[SearchResult]
    ) -> List[SourceRef]:
        """
        è¿½è¸ªç­”æ¡ˆä¸­çš„å†…å®¹æ¥æº
        
        ç®—æ³•ï¼š
        1. æå–ç­”æ¡ˆä¸­çš„å…³é”®å¥å­
        2. ä¸æ¯ä¸ªcontextè®¡ç®—ç›¸ä¼¼åº¦
        3. ç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼åˆ™æ ‡è®°ä¸ºæ¥æº
        """
        sources = []
        answer_sentences = self._split_sentences(answer)
        
        for sent in answer_sentences:
            best_match = None
            best_score = 0
            
            for ctx in contexts:
                # è®¡ç®—å¥å­ä¸contextçš„ç›¸ä¼¼åº¦
                score = self._compute_similarity(sent, ctx.content)
                if score > best_score and score > 0.6:
                    best_score = score
                    best_match = ctx
            
            if best_match:
                sources.append(SourceRef(
                    text=sent,
                    chunk_id=best_match.chunk_id,
                    document=best_match.metadata.get("filename", "unknown"),
                    score=best_score
                ))
        
        return self._deduplicate(sources)
    
    def highlight_sources(
        self, 
        answer: str, 
        sources: List[SourceRef]
    ) -> str:
        """
        åœ¨ç­”æ¡ˆä¸­æ·»åŠ æ¥æºæ ‡è®°
        
        è¾“å‡ºæ ¼å¼ï¼š
        "æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½æ–¹æ³•[1]..."
        """
        # æŒ‰å‡ºç°ä½ç½®æ’åºæ¥æº
        source_map = {}
        for i, source in enumerate(sources, 1):
            if source.chunk_id not in source_map:
                source_map[source.chunk_id] = i
        
        # æ·»åŠ å¼•ç”¨æ ‡è®°
        highlighted = answer
        for source in sources:
            ref_num = source_map[source.chunk_id]
            # åœ¨å¯¹åº”å¥å­åæ·»åŠ å¼•ç”¨
            highlighted = highlighted.replace(
                source.text,
                f"{source.text}[{ref_num}]"
            )
        
        return highlighted
```

---

### 2.6 RAGæµæ°´çº¿ (rag_pipeline.py)

#### 2.6.1 ç±»å›¾

```mermaid
classDiagram
    class RAGPipeline {
        -document_processor: DocumentProcessor
        -embedder: Embedder
        -vector_store: VectorStore
        -retriever: HybridRetriever
        -generator: Generator
        -conversation_manager: ConversationManager
        +index_document(file_path: str) IndexResult
        +query(question: str, session_id: str) QueryResult
        +query_stream(question: str, session_id: str) Iterator
        +delete_document(doc_id: str) bool
        +get_documents() List~Document~
        +clear_conversation(session_id: str) None
    }
    
    class ConversationManager {
        -conversations: Dict~str, List~
        -max_history: int
        +add_message(session_id: str, role: str, content: str) None
        +get_history(session_id: str) List~Dict~
        +clear(session_id: str) None
        +format_history(session_id: str) str
    }
    
    class IndexResult {
        +success: bool
        +document_id: str
        +chunk_count: int
        +message: str
    }
    
    class QueryResult {
        +answer: str
        +sources: List~SourceRef~
        +confidence: float
    }
    
    RAGPipeline --> ConversationManager
    RAGPipeline --> IndexResult
    RAGPipeline --> QueryResult
```

#### 2.6.2 æ ¸å¿ƒæµç¨‹

##### æ–‡æ¡£ç´¢å¼•æµç¨‹

```python
def index_document(self, file_path: str) -> IndexResult:
    """
    ç´¢å¼•æ–‡æ¡£
    
    æµç¨‹ï¼š
    1. éªŒè¯æ–‡ä»¶
    2. åŠ è½½å¹¶è§£ææ–‡æ¡£
    3. æ™ºèƒ½åˆ†å—
    4. å‘é‡åŒ–å¹¶å­˜å‚¨
    5. æ›´æ–°ç¨€ç–ç´¢å¼•
    """
    try:
        # 1. åŠ è½½æ–‡æ¡£
        document = self.document_processor.load_document(file_path)
        
        # 2. æ™ºèƒ½åˆ†å—
        chunks = self.document_processor.process(document)
        
        # 3. å‘é‡åŒ–å­˜å‚¨
        self.vector_store.add_chunks(chunks)
        
        # 4. æ›´æ–°BM25ç´¢å¼•
        self.retriever.sparse_retriever.add_chunks(chunks)
        
        return IndexResult(
            success=True,
            document_id=document.id,
            chunk_count=len(chunks),
            message=f"æˆåŠŸç´¢å¼•æ–‡æ¡£ï¼Œå…±{len(chunks)}ä¸ªæ–‡æœ¬å—"
        )
    except Exception as e:
        return IndexResult(
            success=False,
            document_id=None,
            chunk_count=0,
            message=f"ç´¢å¼•å¤±è´¥: {str(e)}"
        )
```

##### é—®ç­”æµç¨‹

```python
def query(self, question: str, session_id: str = "default") -> QueryResult:
    """
    å¤„ç†ç”¨æˆ·é—®é¢˜
    
    æµç¨‹ï¼š
    1. è·å–å¯¹è¯å†å²
    2. é—®é¢˜æ”¹å†™ï¼ˆå¯é€‰ï¼Œç»“åˆå†å²ç†è§£æŒ‡ä»£ï¼‰
    3. æ··åˆæ£€ç´¢
    4. é‡æ’åº
    5. ç”Ÿæˆç­”æ¡ˆ
    6. æ¥æºè¿½è¸ª
    7. æ›´æ–°å¯¹è¯å†å²
    """
    # 1. è·å–å¯¹è¯å†å²
    history = self.conversation_manager.get_history(session_id)
    
    # 2. æ£€ç´¢ç›¸å…³å†…å®¹
    search_results = self.retriever.retrieve(
        question, 
        top_k=self.config.TOP_K
    )
    
    if not search_results:
        return QueryResult(
            answer="æŠ±æ­‰ï¼Œæ ¹æ®ç°æœ‰çŸ¥è¯†åº“æ— æ³•æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚",
            sources=[],
            confidence=0.0
        )
    
    # 3. æ„å»ºä¸Šä¸‹æ–‡
    contexts = [r.content for r in search_results]
    
    # 4. ç”Ÿæˆç­”æ¡ˆ
    answer = self.generator.generate(
        query=question,
        contexts=contexts,
        history=history
    )
    
    # 5. æ¥æºè¿½è¸ª
    sources = self.generator.source_tracer.trace_sources(
        answer, 
        search_results
    )
    
    # 6. æ·»åŠ å¼•ç”¨æ ‡è®°
    answer_with_refs = self.generator.source_tracer.highlight_sources(
        answer, 
        sources
    )
    
    # 7. æ›´æ–°å¯¹è¯å†å²
    self.conversation_manager.add_message(session_id, "user", question)
    self.conversation_manager.add_message(session_id, "assistant", answer)
    
    # 8. è®¡ç®—ç½®ä¿¡åº¦
    confidence = sum(r.score for r in search_results[:3]) / 3
    
    return QueryResult(
        answer=answer_with_refs,
        sources=sources,
        confidence=confidence
    )
```

---

### 2.7 APIæœåŠ¡ (api.py)

#### 2.7.1 æ¥å£è¯¦ç»†å®šä¹‰

```python
from fastapi import FastAPI, UploadFile, File, HTTPException
from pydantic import BaseModel
from typing import List, Optional

app = FastAPI(title="RAGé—®ç­”ç³»ç»ŸAPI")

# ========== æ•°æ®æ¨¡å‹ ==========

class QueryRequest(BaseModel):
    question: str
    session_id: Optional[str] = "default"
    top_k: Optional[int] = 5

class QueryResponse(BaseModel):
    answer: str
    sources: List[dict]
    confidence: float

class DocumentInfo(BaseModel):
    id: str
    filename: str
    chunk_count: int
    created_at: str

class UploadResponse(BaseModel):
    status: str
    document_id: str
    chunk_count: int
    message: str

# ========== APIç«¯ç‚¹ ==========

@app.post("/api/documents/upload", response_model=UploadResponse)
async def upload_document(file: UploadFile = File(...)):
    """
    ä¸Šä¼ å¹¶ç´¢å¼•æ–‡æ¡£
    
    - æ”¯æŒæ ¼å¼: PDF, TXT, DOCX, Markdown
    - æœ€å¤§æ–‡ä»¶å¤§å°: 50MB
    """
    # éªŒè¯æ–‡ä»¶æ ¼å¼
    if not any(file.filename.endswith(ext) for ext in SUPPORTED_FORMATS):
        raise HTTPException(400, "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
    
    # ä¿å­˜æ–‡ä»¶
    file_path = save_upload_file(file)
    
    # ç´¢å¼•æ–‡æ¡£
    result = rag_pipeline.index_document(file_path)
    
    if not result.success:
        raise HTTPException(500, result.message)
    
    return UploadResponse(
        status="success",
        document_id=result.document_id,
        chunk_count=result.chunk_count,
        message=result.message
    )

@app.get("/api/documents", response_model=List[DocumentInfo])
async def list_documents():
    """è·å–å·²ç´¢å¼•çš„æ–‡æ¡£åˆ—è¡¨"""
    documents = rag_pipeline.get_documents()
    return [
        DocumentInfo(
            id=doc.id,
            filename=doc.filename,
            chunk_count=doc.chunk_count,
            created_at=doc.created_at.isoformat()
        )
        for doc in documents
    ]

@app.delete("/api/documents/{doc_id}")
async def delete_document(doc_id: str):
    """åˆ é™¤æŒ‡å®šæ–‡æ¡£"""
    success = rag_pipeline.delete_document(doc_id)
    if not success:
        raise HTTPException(404, "æ–‡æ¡£ä¸å­˜åœ¨")
    return {"status": "success", "message": "æ–‡æ¡£å·²åˆ é™¤"}

@app.post("/api/qa/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    """
    æäº¤é—®é¢˜å¹¶è·å–ç­”æ¡ˆ
    
    - æ”¯æŒå¤šè½®å¯¹è¯ï¼ˆé€šè¿‡session_idå…³è”ï¼‰
    - è¿”å›ç­”æ¡ˆåŠæ¥æºå¼•ç”¨
    """
    result = rag_pipeline.query(
        question=request.question,
        session_id=request.session_id
    )
    
    return QueryResponse(
        answer=result.answer,
        sources=[
            {
                "chunk_id": s.chunk_id,
                "document": s.document,
                "text": s.text,
                "score": s.score
            }
            for s in result.sources
        ],
        confidence=result.confidence
    )

@app.post("/api/conversation/clear")
async def clear_conversation(session_id: str = "default"):
    """æ¸…ç©ºå¯¹è¯å†å²"""
    rag_pipeline.clear_conversation(session_id)
    return {"status": "success", "message": "å¯¹è¯å†å²å·²æ¸…ç©º"}
```

---

### 2.8 å‰ç«¯åº”ç”¨ (gradio_app.py)

#### 2.8.1 ç•Œé¢è®¾è®¡

```mermaid
flowchart TB
    subgraph layout [ç•Œé¢å¸ƒå±€]
        subgraph left [å·¦ä¾§é¢æ¿]
            UPLOAD[æ–‡æ¡£ä¸Šä¼ åŒº]
            DOCLIST[æ–‡æ¡£åˆ—è¡¨]
            SETTINGS[è®¾ç½®åŒº]
        end
        
        subgraph right [å³ä¾§é¢æ¿]
            CHATBOT[å¯¹è¯åŒº]
            INPUT[è¾“å…¥æ¡†]
            SOURCES[æ¥æºå±•ç¤ºåŒº]
        end
    end
```

#### 2.8.2 ç»„ä»¶è®¾è®¡

```python
import gradio as gr

def create_app():
    with gr.Blocks(title="RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ", theme=gr.themes.Soft()) as app:
        gr.Markdown("# ğŸ¤– RAGå¢å¼ºæ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
        gr.Markdown("ä¸Šä¼ æ–‡æ¡£ï¼Œç„¶ååŸºäºæ–‡æ¡£å†…å®¹è¿›è¡Œé—®ç­”")
        
        with gr.Row():
            # å·¦ä¾§é¢æ¿
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“ æ–‡æ¡£ç®¡ç†")
                
                file_upload = gr.File(
                    label="ä¸Šä¼ æ–‡æ¡£",
                    file_types=[".pdf", ".txt", ".docx", ".md"],
                    file_count="multiple"
                )
                upload_btn = gr.Button("ğŸ“¤ ä¸Šä¼ å¹¶ç´¢å¼•", variant="primary")
                upload_status = gr.Textbox(label="ä¸Šä¼ çŠ¶æ€", interactive=False)
                
                gr.Markdown("### ğŸ“‹ å·²ç´¢å¼•æ–‡æ¡£")
                doc_list = gr.Dataframe(
                    headers=["æ–‡æ¡£å", "å—æ•°", "æ“ä½œ"],
                    label="æ–‡æ¡£åˆ—è¡¨"
                )
                refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨")
                
                gr.Markdown("### âš™ï¸ è®¾ç½®")
                top_k_slider = gr.Slider(1, 10, value=5, step=1, label="æ£€ç´¢æ•°é‡")
                clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯")
            
            # å³ä¾§é¢æ¿
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ’¬ é—®ç­”å¯¹è¯")
                
                chatbot = gr.Chatbot(
                    label="å¯¹è¯å†å²",
                    height=400,
                    show_label=False
                )
                
                with gr.Row():
                    question_input = gr.Textbox(
                        label="è¾“å…¥é—®é¢˜",
                        placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                        scale=4
                    )
                    send_btn = gr.Button("å‘é€", variant="primary", scale=1)
                
                gr.Markdown("### ğŸ“š æ¥æºå¼•ç”¨")
                sources_display = gr.JSON(label="ç­”æ¡ˆæ¥æº")
        
        # äº‹ä»¶ç»‘å®š
        upload_btn.click(
            fn=handle_upload,
            inputs=[file_upload],
            outputs=[upload_status, doc_list]
        )
        
        send_btn.click(
            fn=handle_query,
            inputs=[question_input, chatbot, top_k_slider],
            outputs=[chatbot, sources_display, question_input]
        )
        
        question_input.submit(
            fn=handle_query,
            inputs=[question_input, chatbot, top_k_slider],
            outputs=[chatbot, sources_display, question_input]
        )
        
        clear_btn.click(
            fn=handle_clear,
            inputs=[],
            outputs=[chatbot, sources_display]
        )
        
        refresh_btn.click(
            fn=handle_refresh,
            inputs=[],
            outputs=[doc_list]
        )
    
    return app
```

---

## 3. æ•°æ®ç»“æ„è¯¦ç»†è®¾è®¡

### 3.1 æ ¸å¿ƒæ•°æ®ç±»

```python
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import uuid

@dataclass
class Document:
    """æ–‡æ¡£æ•°æ®ç±»"""
    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    filename: str = ""
    content: str = ""
    format: str = ""
    metadata: Dict = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)
    chunk_count: int = 0

@dataclass
class Chunk:
    """æ–‡æœ¬å—æ•°æ®ç±»"""
    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    document_id: str = ""
    content: str = ""
    start_pos: int = 0
    end_pos: int = 0
    metadata: Dict = field(default_factory=dict)

@dataclass
class SearchResult:
    """æ£€ç´¢ç»“æœæ•°æ®ç±»"""
    chunk_id: str
    content: str
    score: float
    metadata: Dict
    rerank_score: Optional[float] = None

@dataclass
class SourceRef:
    """æ¥æºå¼•ç”¨æ•°æ®ç±»"""
    text: str
    chunk_id: str
    document: str
    score: float
    position: Optional[Tuple[int, int]] = None

@dataclass
class QueryResult:
    """æŸ¥è¯¢ç»“æœæ•°æ®ç±»"""
    answer: str
    sources: List[SourceRef]
    confidence: float
    
@dataclass
class IndexResult:
    """ç´¢å¼•ç»“æœæ•°æ®ç±»"""
    success: bool
    document_id: Optional[str]
    chunk_count: int
    message: str
```

---

## 4. é”™è¯¯å¤„ç†è®¾è®¡

### 4.1 è‡ªå®šä¹‰å¼‚å¸¸

```python
class RAGException(Exception):
    """RAGç³»ç»ŸåŸºç¡€å¼‚å¸¸"""
    pass

class DocumentParseError(RAGException):
    """æ–‡æ¡£è§£æé”™è¯¯"""
    pass

class EmbeddingError(RAGException):
    """åµŒå…¥è®¡ç®—é”™è¯¯"""
    pass

class RetrievalError(RAGException):
    """æ£€ç´¢é”™è¯¯"""
    pass

class GenerationError(RAGException):
    """ç”Ÿæˆé”™è¯¯"""
    pass

class ConfigError(RAGException):
    """é…ç½®é”™è¯¯"""
    pass
```

### 4.2 é”™è¯¯å¤„ç†ç­–ç•¥

```python
def safe_execute(func, *args, fallback=None, **kwargs):
    """å®‰å…¨æ‰§è¡Œå‡½æ•°ï¼Œæ•è·å¼‚å¸¸å¹¶è¿”å›fallback"""
    try:
        return func(*args, **kwargs)
    except RAGException as e:
        logger.error(f"RAGé”™è¯¯: {e}")
        return fallback
    except Exception as e:
        logger.exception(f"æœªçŸ¥é”™è¯¯: {e}")
        return fallback
```

---

## 5. æ—¥å¿—è®¾è®¡

```python
import logging

def setup_logging():
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('rag_system.log'),
            logging.StreamHandler()
        ]
    )
    
    # è®¾ç½®å„æ¨¡å—æ—¥å¿—çº§åˆ«
    logging.getLogger('document_processor').setLevel(logging.INFO)
    logging.getLogger('retriever').setLevel(logging.INFO)
    logging.getLogger('generator').setLevel(logging.INFO)
```

---

## é™„å½•A: é…ç½®æ–‡ä»¶ç¤ºä¾‹

```yaml
# config.yaml
document:
  chunk_size: 512
  chunk_overlap: 64
  max_file_size: 52428800  # 50MB
  supported_formats:
    - .pdf
    - .txt
    - .docx
    - .md

embedding:
  model: "BAAI/bge-m3"
  dimension: 1024
  batch_size: 32

retrieval:
  top_k: 5
  rerank_top_k: 10
  hybrid_alpha: 0.7

generation:
  model: "qwen2.5:7b"
  max_tokens: 1024
  temperature: 0.7

server:
  host: "0.0.0.0"
  port: 7860
```

## é™„å½•B: ä¾èµ–åˆ—è¡¨

```
# requirements.txt
langchain>=0.1.0
langchain-community>=0.0.10
chromadb>=0.4.0
sentence-transformers>=2.2.0
transformers>=4.35.0
torch>=2.0.0
gradio>=4.0.0
fastapi>=0.100.0
uvicorn>=0.23.0
python-multipart>=0.0.6
pypdf>=3.0.0
python-docx>=0.8.11
markdown>=3.4.0
rank-bm25>=0.2.2
numpy>=1.24.0
pydantic>=2.0.0
ollama>=0.1.0
```
