"""
ç”Ÿæˆæ¨¡å—

è´Ÿè´£æ„å»ºPromptå’Œè°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
"""

import logging
import re
from typing import List, Dict, Optional, Iterator, Tuple
from dataclasses import dataclass

from ..retrieval.embedder import SearchResult

logger = logging.getLogger(__name__)


@dataclass
class SourceRef:
    """æ¥æºå¼•ç”¨æ•°æ®ç±»"""
    text: str
    chunk_id: str
    document: str
    score: float
    position: Optional[Tuple[int, int]] = None


@dataclass 
class GenerationResult:
    """ç”Ÿæˆç»“æœæ•°æ®ç±»"""
    answer: str
    sources: List[SourceRef]
    confidence: float


# ==================== Promptæ¨¡æ¿ ====================

# RAGæ¨¡å¼çš„ç³»ç»Ÿæç¤ºè¯ï¼ˆæœ‰çŸ¥è¯†åº“æ—¶ä½¿ç”¨ï¼‰
SYSTEM_PROMPT_RAG_CN = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„å‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è§„åˆ™ï¼š
1. ä¼˜å…ˆæ ¹æ®å‚è€ƒèµ„æ–™å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. å¦‚æœå‚è€ƒèµ„æ–™ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜"
3. å›ç­”æ—¶å¼•ç”¨æ¥æºï¼Œä½¿ç”¨[1], [2]ç­‰æ ‡è®°
4. å›ç­”è¦å‡†ç¡®ã€ç®€æ´ã€æœ‰æ¡ç†
5. ä½¿ç”¨ä¸ç”¨æˆ·é—®é¢˜ç›¸åŒçš„è¯­è¨€å›ç­”"""

# é€šç”¨å¯¹è¯æ¨¡å¼çš„ç³»ç»Ÿæç¤ºè¯ï¼ˆæ— çŸ¥è¯†åº“æ—¶ä½¿ç”¨ï¼‰
SYSTEM_PROMPT_CHAT_CN = """ä½ æ˜¯ä¸€ä¸ªå‹å¥½ã€ä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·ç”¨å‡†ç¡®ã€ç®€æ´ã€æœ‰æ¡ç†çš„æ–¹å¼å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è§„åˆ™ï¼š
1. å›ç­”è¦å‡†ç¡®ã€ç®€æ´ã€æœ‰æ¡ç†
2. ä½¿ç”¨ä¸ç”¨æˆ·é—®é¢˜ç›¸åŒçš„è¯­è¨€å›ç­”
3. å¦‚æœä¸ç¡®å®šç­”æ¡ˆï¼Œè¯·è¯šå®è¯´æ˜
4. ä¿æŒå‹å¥½å’Œä¸“ä¸šçš„æ€åº¦"""

# å…¼å®¹æ—§ç‰ˆæœ¬çš„åˆ«å
SYSTEM_PROMPT_CN = SYSTEM_PROMPT_RAG_CN

SYSTEM_PROMPT_EN = """You are a professional Q&A assistant. Please answer user questions based on the provided reference materials.

Rules:
1. Prioritize answering based on reference materials, do not make up information
2. If there is no relevant content in the reference materials, clearly state "Unable to answer this question based on available information"
3. Cite sources using [1], [2] markers when answering
4. Answers should be accurate, concise, and well-organized
5. Answer in the same language as the user's question"""

SYSTEM_PROMPT_CHAT_EN = """You are a friendly and professional AI assistant. Please answer user questions accurately, concisely, and in an organized manner.

Rules:
1. Answers should be accurate, concise, and well-organized
2. Answer in the same language as the user's question
3. If uncertain about an answer, be honest about it
4. Maintain a friendly and professional attitude"""

RAG_TEMPLATE = """å‚è€ƒèµ„æ–™ï¼š
{contexts}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·æ ¹æ®å‚è€ƒèµ„æ–™å›ç­”ä¸Šè¿°é—®é¢˜ï¼š"""

RAG_TEMPLATE_WITH_HISTORY = """å‚è€ƒèµ„æ–™ï¼š
{contexts}

å¯¹è¯å†å²ï¼š
{history}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·æ ¹æ®å‚è€ƒèµ„æ–™å’Œå¯¹è¯ä¸Šä¸‹æ–‡å›ç­”ä¸Šè¿°é—®é¢˜ï¼š"""


class PromptBuilder:
    """Promptæ„å»ºå™¨"""
    
    def __init__(
        self,
        system_prompt: str = None,
        rag_template: str = None,
        rag_template_with_history: str = None
    ):
        self.system_prompt = system_prompt or SYSTEM_PROMPT_CN
        self.rag_template = rag_template or RAG_TEMPLATE
        self.rag_template_with_history = rag_template_with_history or RAG_TEMPLATE_WITH_HISTORY
    
    def build_contexts(
        self,
        search_results: List[SearchResult],
        max_contexts: int = 5
    ) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡æ–‡æœ¬"""
        if not search_results:
            return "ï¼ˆæ— ç›¸å…³å‚è€ƒèµ„æ–™ï¼‰"
        
        contexts = []
        for i, result in enumerate(search_results[:max_contexts], 1):
            source = result.metadata.get("filename", "æœªçŸ¥æ¥æº")
            contexts.append(f"[{i}] æ¥æºï¼š{source}\n{result.content}")
        
        return "\n\n".join(contexts)
    
    def build_history(self, history: List[Dict]) -> str:
        """æ„å»ºå¯¹è¯å†å²æ–‡æœ¬"""
        if not history:
            return ""
        
        lines = []
        for msg in history[-6:]:
            role = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
            lines.append(f"{role}: {msg['content']}")
        
        return "\n".join(lines)
    
    def build_prompt(
        self,
        query: str,
        search_results: List[SearchResult],
        history: List[Dict] = None
    ) -> str:
        """æ„å»ºå®Œæ•´çš„Prompt"""
        contexts = self.build_contexts(search_results)
        
        if history:
            history_text = self.build_history(history)
            prompt = self.rag_template_with_history.format(
                contexts=contexts,
                history=history_text,
                query=query
            )
        else:
            prompt = self.rag_template.format(
                contexts=contexts,
                query=query
            )
        
        return prompt
    
    def build_messages(
        self,
        query: str,
        search_results: List[SearchResult],
        history: List[Dict] = None
    ) -> List[Dict]:
        """æ„å»ºæ¶ˆæ¯åˆ—è¡¨æ ¼å¼ï¼ˆç”¨äºChat APIï¼‰"""
        if search_results:
            system_prompt = self.system_prompt
        else:
            system_prompt = SYSTEM_PROMPT_CHAT_CN
        
        messages = [{"role": "system", "content": system_prompt}]
        
        if history:
            for msg in history[-6:]:
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
        
        if search_results:
            contexts = self.build_contexts(search_results)
            user_message = f"å‚è€ƒèµ„æ–™ï¼š\n{contexts}\n\né—®é¢˜ï¼š{query}"
        else:
            user_message = query
        messages.append({"role": "user", "content": user_message})
        
        return messages


class SourceTracer:
    """
    æ¥æºè¿½è¸ªå™¨ - åˆ›æ–°ç‚¹
    
    è¿½è¸ªç­”æ¡ˆä¸­çš„å†…å®¹æ¥æº
    """
    
    def __init__(self, similarity_threshold: float = 0.5):
        self.similarity_threshold = similarity_threshold
    
    def trace_sources(
        self,
        answer: str,
        search_results: List[SearchResult]
    ) -> List[SourceRef]:
        """è¿½è¸ªç­”æ¡ˆä¸­çš„å†…å®¹æ¥æº"""
        sources = []
        
        citation_pattern = r'\[(\d+)\]'
        citations = re.findall(citation_pattern, answer)
        
        for citation in set(citations):
            idx = int(citation) - 1
            if 0 <= idx < len(search_results):
                result = search_results[idx]
                sources.append(SourceRef(
                    text=f"[{citation}]",
                    chunk_id=result.chunk_id,
                    document=result.metadata.get("filename", "unknown"),
                    score=result.score
                ))
        
        return sources
    
    def highlight_sources(
        self,
        answer: str,
        sources: List[SourceRef]
    ) -> str:
        """
        åœ¨ç­”æ¡ˆä¸­é«˜äº®æ˜¾ç¤ºæ¥æº
        
        æ³¨æ„ï¼šå½“å‰å®ç°ä»…éªŒè¯å¼•ç”¨æ ‡è®°æ˜¯å¦å­˜åœ¨ï¼Œ
        æœªæ¥å¯æ‰©å±•ä¸ºHTML/Markdowné«˜äº®æ ¼å¼
        """
        # éªŒè¯ç­”æ¡ˆä¸­å·²æœ‰å¼•ç”¨æ ‡è®°
        if re.search(r'\[\d+\]', answer):
            return answer
        
        # å¦‚æœæ²¡æœ‰å¼•ç”¨æ ‡è®°ä½†æœ‰æ¥æºï¼Œå¯ä»¥åœ¨æœ«å°¾æ·»åŠ æ¥æºè¯´æ˜
        if sources:
            source_notes = "\n\n**å‚è€ƒæ¥æºï¼š**\n"
            for src in sources:
                source_notes += f"- {src.text}: {src.document}\n"
            return answer + source_notes
        
        return answer
    
    def format_sources(
        self,
        search_results: List[SearchResult]
    ) -> List[Dict]:
        """æ ¼å¼åŒ–æ¥æºä¿¡æ¯"""
        return [
            {
                "index": i + 1,
                "document": result.metadata.get("filename", "unknown"),
                "content": result.content[:200] + "..." if len(result.content) > 200 else result.content,
                "score": round(result.score, 3)
            }
            for i, result in enumerate(search_results)
        ]


class Generator:
    """
    ç­”æ¡ˆç”Ÿæˆå™¨
    
    æ”¯æŒOllamaå’ŒOpenAIä¸¤ç§åç«¯
    """
    
    def __init__(
        self,
        provider: str = "ollama",
        model: str = "qwen2.5:7b",
        base_url: str = "http://localhost:11434",
        api_key: str = None,
        temperature: float = 0.7,
        max_tokens: int = 1024
    ):
        self.provider = provider
        self.model = model
        self.base_url = base_url
        self.api_key = api_key
        self.temperature = temperature
        self.max_tokens = max_tokens
        
        self.prompt_builder = PromptBuilder()
        self.source_tracer = SourceTracer()
        
        self._client = None
        self._init_client()
    
    def _init_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        if self.provider == "ollama":
            self._init_ollama()
        elif self.provider == "openai":
            self._init_openai()
        else:
            logger.warning(f"æœªçŸ¥çš„æä¾›å•†: {self.provider}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
    
    def _init_ollama(self):
        """åˆå§‹åŒ–Ollamaå®¢æˆ·ç«¯"""
        try:
            import ollama
            self._client = ollama.Client(host=self.base_url)
            logger.info(f"Ollamaå®¢æˆ·ç«¯å·²åˆ›å»º: {self.base_url}")
            # ä¸åœ¨åˆå§‹åŒ–æ—¶æ£€æŸ¥è¿æ¥ï¼Œé¿å…æœåŠ¡æš‚æ—¶ä¸å¯ç”¨å¯¼è‡´æ°¸ä¹…è¿›å…¥mockæ¨¡å¼
            # è¿æ¥é—®é¢˜å°†åœ¨å®é™…è°ƒç”¨æ—¶å¤„ç†
        except ImportError:
            logger.warning("ollamaåº“æœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
            self._client = None
    
    def _init_openai(self):
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼ˆæ”¯æŒOpenAIå…¼å®¹APIï¼Œå¦‚DeepSeekç­‰ï¼‰"""
        try:
            from openai import OpenAI
            if self.api_key:
                client_kwargs = {"api_key": self.api_key}
                if self.base_url and self.base_url.strip():
                    client_kwargs["base_url"] = self.base_url
                    logger.info(f"ä½¿ç”¨è‡ªå®šä¹‰APIåœ°å€: {self.base_url}")
                
                self._client = OpenAI(**client_kwargs)
                logger.info("OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.warning("æœªæä¾›OpenAI API Key")
                self._client = None
        except ImportError:
            logger.warning("openaiåº“æœªå®‰è£…")
            self._client = None
    
    def generate(
        self,
        query: str,
        search_results: List[SearchResult],
        history: List[Dict] = None
    ) -> GenerationResult:
        """ç”Ÿæˆç­”æ¡ˆ"""
        messages = self.prompt_builder.build_messages(
            query, search_results, history
        )
        
        if self._client is None:
            answer = self._generate_mock(query, search_results)
        elif self.provider == "ollama":
            answer = self._generate_ollama(messages)
        elif self.provider == "openai":
            answer = self._generate_openai(messages)
        else:
            answer = self._generate_mock(query, search_results)
        
        sources = self.source_tracer.trace_sources(answer, search_results)
        
        if search_results:
            confidence = sum(r.score for r in search_results[:3]) / min(3, len(search_results))
        else:
            confidence = 0.0
        
        return GenerationResult(
            answer=answer,
            sources=sources,
            confidence=confidence
        )
    
    def _generate_ollama(self, messages: List[Dict]) -> str:
        """ä½¿ç”¨Ollamaç”Ÿæˆ"""
        try:
            response = self._client.chat(
                model=self.model,
                messages=messages,
                options={
                    "temperature": self.temperature,
                    "num_predict": self.max_tokens
                }
            )
            return response["message"]["content"]
        except (ConnectionError, OSError) as e:
            # æ•è·è¿æ¥é”™è¯¯å’Œç½‘ç»œé”™è¯¯
            logger.error(f"Ollamaè¿æ¥å¤±è´¥: {e}")
            raise ConnectionError(f"OllamaæœåŠ¡è¿æ¥å¤±è´¥ ({self.base_url}): {str(e)}")
        except Exception as e:
            error_msg = str(e).lower()
            logger.error(f"Ollamaç”Ÿæˆå¤±è´¥: {e}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œ/è¿æ¥ç›¸å…³é”™è¯¯
            network_error_keywords = [
                'connection', 'connect', 'refused', 'timeout',
                'errno', 'address', 'network', 'unreachable',
                'socket', 'host', 'port'
            ]
            
            if any(keyword in error_msg for keyword in network_error_keywords):
                raise ConnectionError(f"OllamaæœåŠ¡è¿æ¥å¤±è´¥ ({self.base_url}): {str(e)}")
            elif "model" in error_msg and "not found" in error_msg:
                return f"âš ï¸ æ¨¡å‹ '{self.model}' æœªæ‰¾åˆ°ã€‚è¯·å…ˆè¿è¡Œï¼š`ollama pull {self.model}`"
            
            # å…¶ä»–æœªçŸ¥é”™è¯¯
            return f"ç”Ÿæˆå¤±è´¥ï¼š{str(e)}"
    
    def _generate_openai(self, messages: List[Dict]) -> str:
        """ä½¿ç”¨OpenAIç”Ÿæˆ"""
        try:
            response = self._client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"OpenAIç”Ÿæˆå¤±è´¥: {e}")
            return f"ç”Ÿæˆå¤±è´¥ï¼š{str(e)}"
    
    def _generate_mock(
        self,
        query: str,
        search_results: List[SearchResult]
    ) -> str:
        """æ¨¡æ‹Ÿç”Ÿæˆï¼ˆç”¨äºæµ‹è¯•æˆ–æ— LLMæ—¶ï¼‰"""
        if not search_results:
            return f"""æ‚¨å¥½ï¼æ‚¨çš„é—®é¢˜æ˜¯ï¼š"{query}"

å½“å‰ç³»ç»Ÿå¤„äºæ¨¡æ‹Ÿæ¨¡å¼ï¼Œæ— æ³•æä¾›çœŸå®å›ç­”ã€‚

è¯·åœ¨å·¦ä¾§"LLMé…ç½®"åŒºåŸŸé…ç½®Ollamaæˆ–OpenAIåå³å¯æ­£å¸¸ä½¿ç”¨é—®ç­”åŠŸèƒ½ã€‚

ğŸ’¡ æç¤ºï¼š
- ä½¿ç”¨Ollamaï¼šç¡®ä¿OllamaæœåŠ¡å·²å¯åŠ¨ï¼Œå¹¶å¡«å†™æ­£ç¡®çš„æ¨¡å‹åç§°å’Œåœ°å€
- ä½¿ç”¨OpenAIï¼šå¡«å†™æ‚¨çš„API Keyå’Œæ¨¡å‹åç§°"""
        
        top_result = search_results[0]
        source = top_result.metadata.get("filename", "æœªçŸ¥æ¥æº")
        
        answer = f"""æ ¹æ®å‚è€ƒèµ„æ–™ï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š

{top_result.content[:500]}

[1] æ¥æºï¼š{source}

æ³¨æ„ï¼šå½“å‰ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ã€‚å¦‚éœ€æ›´å‡†ç¡®çš„å›ç­”ï¼Œè¯·é…ç½®Ollamaæˆ–OpenAIã€‚"""
        
        return answer
    
    def generate_stream(
        self,
        query: str,
        search_results: List[SearchResult],
        history: List[Dict] = None
    ) -> Iterator[str]:
        """æµå¼ç”Ÿæˆç­”æ¡ˆ"""
        messages = self.prompt_builder.build_messages(
            query, search_results, history
        )
        
        if self._client is None or self.provider not in ["ollama", "openai"]:
            mock_response = self._generate_mock(query, search_results)
            for char in mock_response:
                yield char
            return
        
        try:
            if self.provider == "ollama":
                for chunk in self._stream_ollama(messages):
                    yield chunk
            elif self.provider == "openai":
                for chunk in self._stream_openai(messages):
                    yield chunk
        except ConnectionError as e:
            # è¿æ¥é”™è¯¯ - é‡æ–°æŠ›å‡ºï¼Œè®©ä¸Šå±‚å¤„ç†å‹å¥½æç¤º
            raise
        except Exception as e:
            error_msg = str(e).lower()
            logger.error(f"æµå¼ç”Ÿæˆå¤±è´¥: {e}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œç›¸å…³é”™è¯¯
            network_keywords = [
                'connection', 'connect', 'refused', 'timeout',
                'errno', 'address', 'network', 'unreachable',
                'socket', 'host', 'port'
            ]
            
            if any(keyword in error_msg for keyword in network_keywords):
                # ç½‘ç»œé”™è¯¯ - æŠ›å‡ºConnectionErrorè®©ä¸Šå±‚å¤„ç†
                raise ConnectionError(f"æœåŠ¡è¿æ¥å¤±è´¥: {str(e)}")
            elif "model" in error_msg and "not found" in error_msg:
                yield f"âš ï¸ æ¨¡å‹ '{self.model}' æœªæ‰¾åˆ°ã€‚è¯·å…ˆè¿è¡Œï¼š`ollama pull {self.model}`"
            else:
                yield f"ç”Ÿæˆå¤±è´¥ï¼š{str(e)}"
    
    def _stream_ollama(self, messages: List[Dict]) -> Iterator[str]:
        """Ollamaæµå¼ç”Ÿæˆ"""
        try:
            response = self._client.chat(
                model=self.model,
                messages=messages,
                stream=True,
                options={
                    "temperature": self.temperature,
                    "num_predict": self.max_tokens
                }
            )
            for chunk in response:
                if "message" in chunk and "content" in chunk["message"]:
                    yield chunk["message"]["content"]
        except (ConnectionError, OSError) as e:
            # æ•è·è¿æ¥é”™è¯¯å’Œç½‘ç»œé”™è¯¯
            logger.error(f"Ollamaæµå¼è¿æ¥å¤±è´¥: {e}")
            raise ConnectionError(f"OllamaæœåŠ¡è¿æ¥å¤±è´¥ ({self.base_url}): {str(e)}")
        except Exception as e:
            error_msg = str(e).lower()
            logger.error(f"Ollamaæµå¼ç”Ÿæˆå¤±è´¥: {e}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œ/è¿æ¥ç›¸å…³é”™è¯¯
            network_error_keywords = [
                'connection', 'connect', 'refused', 'timeout',
                'errno', 'address', 'network', 'unreachable',
                'socket', 'host', 'port'
            ]
            
            if any(keyword in error_msg for keyword in network_error_keywords):
                raise ConnectionError(f"OllamaæœåŠ¡è¿æ¥å¤±è´¥ ({self.base_url}): {str(e)}")
            
            # å…¶ä»–é”™è¯¯ç»§ç»­æŠ›å‡º
            raise
    
    def _stream_openai(self, messages: List[Dict]) -> Iterator[str]:
        """OpenAIæµå¼ç”Ÿæˆ"""
        response = self._client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
            stream=True
        )
        for chunk in response:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
