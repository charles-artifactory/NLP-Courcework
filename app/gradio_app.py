"""
Gradioå‰ç«¯åº”ç”¨

æä¾›å‹å¥½çš„Webäº¤äº’ç•Œé¢
"""

from src.core.pipeline import get_pipeline, RAGPipeline
from src.config import get_config
import logging
from pathlib import Path
from typing import List, Tuple, Generator, Dict

import gradio as gr

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))


logger = logging.getLogger(__name__)


# ==================== äº‹ä»¶å¤„ç†å‡½æ•° ====================

def handle_load_example(progress=gr.Progress()) -> Tuple[str, List[List], str]:
    """
    åŠ è½½ç¤ºä¾‹æ–‡æ¡£å¹¶æä¾›ç¤ºä¾‹é—®é¢˜
    
    Args:
        progress: Gradioè¿›åº¦æ¡å¯¹è±¡
    
    Returns:
        Tuple[str, List[List], str]: (ä¸Šä¼ çŠ¶æ€, æ–‡æ¡£åˆ—è¡¨, ç¤ºä¾‹é—®é¢˜æç¤º)
    """
    # ç¤ºä¾‹æ–‡æ¡£è·¯å¾„
    example_file = Path(__file__).parent.parent / "data" / "examples" / "sample_document.md"
    
    if not example_file.exists():
        return "âŒ ç¤ºä¾‹æ–‡ä»¶ä¸å­˜åœ¨", get_document_list(), ""
    
    progress(0, desc="ğŸ“¦ åˆå§‹åŒ–ç³»ç»Ÿ...")
    pipeline = get_pipeline()
    pipeline.initialize()
    
    try:
        progress(0.3, desc="ğŸ“„ åŠ è½½ç¤ºä¾‹æ–‡æ¡£...")
        result = pipeline.index_document(str(example_file))
        
        progress(0.8, desc="ğŸ”¢ ç”Ÿæˆå‘é‡ç´¢å¼•...")
        
        if result.success:
            progress(1.0, desc="âœ… åŠ è½½å®Œæˆï¼")
            
            sample_questions = """ğŸ“ **ç¤ºä¾‹é—®é¢˜å»ºè®®**ï¼ˆå¤åˆ¶ç²˜è´´åˆ°ä¸‹æ–¹è¾“å…¥æ¡†ï¼‰ï¼š

ğŸ”¹ ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿå®ƒæœ‰å“ªäº›ä¸»è¦ç‰¹å¾ï¼Ÿ
ğŸ”¹ What are the main types of machine learning?
ğŸ”¹ è¯·è§£é‡ŠRAGæŠ€æœ¯çš„å·¥ä½œåŸç†
ğŸ”¹ æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
ğŸ”¹ What are the advantages of using RAG technology?
ğŸ”¹ å­¦ä¹ AIéœ€è¦ä»€ä¹ˆåŸºç¡€çŸ¥è¯†ï¼Ÿ"""
            
            status = f"âœ… ç¤ºä¾‹æ–‡æ¡£å·²åŠ è½½: {result.chunk_count}ä¸ªæ–‡æœ¬å—\n\n{sample_questions}"
            return status, get_document_list(), ""
        else:
            return f"âŒ åŠ è½½å¤±è´¥: {result.message}", get_document_list(), ""
    except Exception as e:
        logger.error(f"åŠ è½½ç¤ºä¾‹å¤±è´¥: {e}")
        return f"âŒ åŠ è½½ç¤ºä¾‹å¤±è´¥: {str(e)}", get_document_list(), ""


def handle_upload(files: List, progress=gr.Progress()) -> Tuple[str, List[List]]:
    """
    å¤„ç†æ–‡ä»¶ä¸Šä¼ 

    Args:
        files: ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
        progress: Gradioè¿›åº¦æ¡å¯¹è±¡

    Returns:
        Tuple[str, List[List]]: (çŠ¶æ€æ¶ˆæ¯, æ–‡æ¡£åˆ—è¡¨)
    """
    if not files:
        return "è¯·é€‰æ‹©è¦ä¸Šä¼ çš„æ–‡ä»¶", get_document_list()

    pipeline = get_pipeline()
    
    # æ˜¾ç¤ºåˆå§‹åŒ–è¿›åº¦
    progress(0, desc="ğŸ“¦ åˆå§‹åŒ–ç³»ç»Ÿ...")
    pipeline.initialize()

    results = []
    total_files = len(files)
    
    for idx, file in enumerate(files):
        try:
            # file å¯èƒ½æ˜¯ tempfile è·¯å¾„
            file_path = file.name if hasattr(file, 'name') else str(file)
            filename = Path(file_path).name
            
            # æ›´æ–°è¿›åº¦ï¼šæ˜¾ç¤ºå½“å‰å¤„ç†çš„æ–‡ä»¶
            progress((idx / total_files), desc=f"ğŸ“„ å¤„ç†æ–‡ä»¶ ({idx+1}/{total_files}): {filename}")
            
            # ç´¢å¼•è¿‡ç¨‹çš„å­è¿›åº¦
            progress((idx + 0.3) / total_files, desc=f"ğŸ“ åˆ‡ç‰‡æ–‡æ¡£: {filename}")
            result = pipeline.index_document(file_path)
            
            progress((idx + 0.7) / total_files, desc=f"ğŸ”¢ ç”Ÿæˆå‘é‡: {filename}")

            if result.success:
                results.append(f"âœ… {filename}: {result.chunk_count}ä¸ªæ–‡æœ¬å—")
            else:
                results.append(f"âŒ {filename}: {result.message}")
                
        except Exception as e:
            results.append(f"âŒ {filename if 'filename' in locals() else 'æœªçŸ¥æ–‡ä»¶'}: {str(e)}")
    
    # å®Œæˆ
    progress(1.0, desc="âœ… ç´¢å¼•å®Œæˆï¼")
    
    status = "\n".join(results)
    doc_list = get_document_list()

    return status, doc_list


def get_document_list() -> List[List]:
    """
    è·å–æ–‡æ¡£åˆ—è¡¨

    Returns:
        List[List]: æ–‡æ¡£åˆ—è¡¨æ•°æ®
    """
    try:
        pipeline = get_pipeline()
        pipeline.initialize()
        documents = pipeline.get_documents()

        return [
            [doc.get("filename", "æœªçŸ¥"), doc.get("chunk_count", 0), doc.get("id", "")]
            for doc in documents
        ]
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
        return []


def handle_query(
    question: str,
    history: List[dict],
    top_k: int
) -> Generator[Tuple[List[dict], dict, str], None, None]:
    """
    å¤„ç†ç”¨æˆ·é—®é¢˜ï¼ˆéæµå¼ä¹Ÿç”¨ç”Ÿæˆå™¨æ¨¡å¼ä»¥æ”¯æŒå³æ—¶æ˜¾ç¤ºï¼‰

    Args:
        question: ç”¨æˆ·é—®é¢˜
        history: å¯¹è¯å†å²
        top_k: æ£€ç´¢æ•°é‡

    Yields:
        Tuple[List[dict], dict, str]: (æ›´æ–°åçš„å†å², æ¥æºä¿¡æ¯, æ¸…ç©ºçš„è¾“å…¥æ¡†)
    """
    if not question.strip():
        yield history, {}, ""
        return

    # ç«‹å³æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯å’ŒloadingçŠ¶æ€
    history = history or []
    history.append({"role": "user", "content": question})
    history.append({"role": "assistant", "content": "ğŸ¤” æ€è€ƒä¸­..."})
    yield history, {}, ""
    
    pipeline = get_pipeline()
    pipeline.initialize()

    try:
        result = pipeline.query(
            question=question,
            session_id="gradio_session",
            top_k=top_k
        )

        # æ›´æ–°åŠ©æ‰‹å›ç­”
        history[-1]["content"] = result.answer

        # æ ¼å¼åŒ–æ¥æº
        sources_display = {
            "answer_confidence": f"{result.confidence:.2%}",
            "sources": result.sources
        }

        yield history, sources_display, ""

    except ConnectionError as e:
        # ä¸“é—¨æ•è·è¿æ¥é”™è¯¯
        logger.error(f"LLMè¿æ¥å¤±è´¥: {e}")
        error_msg = f"""âŒ **LLMæœåŠ¡è¿æ¥å¤±è´¥**

**å¯èƒ½åŸå› **ï¼š
1. ğŸ”´ OllamaæœåŠ¡æœªå¯åŠ¨
2. âš ï¸ Ollamaåœ°å€é…ç½®é”™è¯¯
3. ğŸŒ ç½‘ç»œè¿æ¥é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**ï¼š

ğŸ’¡ **æ–¹æ¡ˆ1ï¼šå¯åŠ¨OllamaæœåŠ¡**ï¼ˆæ¨èæœ¬åœ°ä½¿ç”¨ï¼‰
```bash
# åœ¨æ–°ç»ˆç«¯æ‰§è¡Œ
ollama serve
```

ğŸ’¡ **æ–¹æ¡ˆ2ï¼šåˆ‡æ¢åˆ°OpenAIæ¨¡å¼**ï¼ˆæ— éœ€æœ¬åœ°æœåŠ¡ï¼‰
1. åœ¨å·¦ä¾§æ‰¾åˆ° **"ğŸ¤– LLM é…ç½®"** åŒºåŸŸ
2. **LLM æä¾›å•†** é€‰æ‹© `openai`
3. å¡«å†™é…ç½®ï¼š
   - **API Base URL**: `https://api.deepseek.com/v1` æˆ– `https://api.openai.com/v1`
   - **API Key**: ä½ çš„APIå¯†é’¥
   - **æ¨¡å‹åç§°**: `deepseek-chat` æˆ– `gpt-3.5-turbo`
4. ç‚¹å‡» **"ğŸ’¾ ä¿å­˜LLMé…ç½®"**
5. é‡æ–°æé—®å³å¯

ğŸ“ **è¯¦ç»†é”™è¯¯ä¿¡æ¯**: {str(e)}"""
        
        history[-1]["content"] = error_msg
        yield history, {"error": "è¿æ¥å¤±è´¥"}, ""
        
    except Exception as e:
        logger.error(f"é—®ç­”å¤±è´¥: {e}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œç›¸å…³é”™è¯¯
        error_str = str(e).lower()
        network_keywords = [
            'connection', 'connect', 'refused', 'timeout', 
            'errno', 'address', 'network', 'unreachable',
            'socket', 'host', 'port', 'ollama'
        ]
        
        if any(keyword in error_str for keyword in network_keywords):
            error_msg = f"""âŒ **LLMæœåŠ¡è¿æ¥å¤±è´¥**

**å¯èƒ½åŸå› **ï¼š
1. ğŸ”´ OllamaæœåŠ¡æœªå¯åŠ¨
2. âš ï¸ Ollamaåœ°å€é…ç½®é”™è¯¯
3. ğŸŒ ç½‘ç»œè¿æ¥é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**ï¼š

ğŸ’¡ **æ–¹æ¡ˆ1ï¼šå¯åŠ¨OllamaæœåŠ¡**
```bash
ollama serve
```

ğŸ’¡ **æ–¹æ¡ˆ2ï¼šåˆ‡æ¢åˆ°OpenAIæ¨¡å¼**
åœ¨å·¦ä¾§"ğŸ¤– LLMé…ç½®"åŒºåŸŸï¼š
- é€‰æ‹© `openai` æä¾›å•†
- å¡«å†™API Keyå’Œæ¨¡å‹åç§°
- ç‚¹å‡»"ä¿å­˜LLMé…ç½®"

ğŸ“ **é”™è¯¯è¯¦æƒ…**: {str(e)}"""
        else:
            error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
        
        history[-1]["content"] = error_msg
        yield history, {"error": str(e)}, ""


def handle_query_stream(
    question: str,
    history: List[dict],
    top_k: int
) -> Generator[Tuple[List[dict], dict, str], None, None]:
    """
    æµå¼å¤„ç†ç”¨æˆ·é—®é¢˜

    Args:
        question: ç”¨æˆ·é—®é¢˜
        history: å¯¹è¯å†å²
        top_k: æ£€ç´¢æ•°é‡

    Yields:
        Tuple[List[dict], dict, str]: (æ›´æ–°åçš„å†å², æ¥æºä¿¡æ¯, æ¸…ç©ºçš„è¾“å…¥æ¡†)
    """
    if not question.strip():
        yield history, {}, ""
        return

    # ç«‹å³æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯å’ŒloadingçŠ¶æ€
    history = history or []
    history.append({"role": "user", "content": question})
    history.append({"role": "assistant", "content": "ğŸ” æ£€ç´¢ä¸­..."})
    yield history, {}, ""
    
    pipeline = get_pipeline()
    pipeline.initialize()

    try:
        full_answer = ""
        sources = []

        for chunk, src in pipeline.query_stream(
            question=question,
            session_id="gradio_session",
            top_k=top_k
        ):
            full_answer += chunk
            sources = src
            history[-1]["content"] = full_answer

            sources_display = {
                "sources": sources
            }

            yield history, sources_display, ""

    except ConnectionError as e:
        # ä¸“é—¨æ•è·è¿æ¥é”™è¯¯
        logger.error(f"LLMè¿æ¥å¤±è´¥: {e}")
        error_msg = f"""âŒ **LLMæœåŠ¡è¿æ¥å¤±è´¥**

**å¯èƒ½åŸå› **ï¼š
1. ğŸ”´ OllamaæœåŠ¡æœªå¯åŠ¨
2. âš ï¸ Ollamaåœ°å€é…ç½®é”™è¯¯
3. ğŸŒ ç½‘ç»œè¿æ¥é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**ï¼š

ğŸ’¡ **æ–¹æ¡ˆ1ï¼šå¯åŠ¨OllamaæœåŠ¡**ï¼ˆæ¨èæœ¬åœ°ä½¿ç”¨ï¼‰
```bash
# åœ¨æ–°ç»ˆç«¯æ‰§è¡Œ
ollama serve
```

ğŸ’¡ **æ–¹æ¡ˆ2ï¼šåˆ‡æ¢åˆ°OpenAIæ¨¡å¼**ï¼ˆæ— éœ€æœ¬åœ°æœåŠ¡ï¼‰
1. åœ¨å·¦ä¾§æ‰¾åˆ° **"ğŸ¤– LLM é…ç½®"** åŒºåŸŸ
2. **LLM æä¾›å•†** é€‰æ‹© `openai`
3. å¡«å†™é…ç½®ï¼š
   - **API Base URL**: `https://api.deepseek.com/v1` æˆ– `https://api.openai.com/v1`
   - **API Key**: ä½ çš„APIå¯†é’¥
   - **æ¨¡å‹åç§°**: `deepseek-chat` æˆ– `gpt-3.5-turbo`
4. ç‚¹å‡» **"ğŸ’¾ ä¿å­˜LLMé…ç½®"**
5. é‡æ–°æé—®å³å¯

ğŸ“ **è¯¦ç»†é”™è¯¯ä¿¡æ¯**: {str(e)}"""
        
        history[-1]["content"] = error_msg
        yield history, {"error": "è¿æ¥å¤±è´¥"}, ""
        
    except Exception as e:
        logger.error(f"æµå¼é—®ç­”å¤±è´¥: {e}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œç›¸å…³é”™è¯¯
        error_str = str(e).lower()
        network_keywords = [
            'connection', 'connect', 'refused', 'timeout',
            'errno', 'address', 'network', 'unreachable',
            'socket', 'host', 'port', 'ollama'
        ]
        
        if any(keyword in error_str for keyword in network_keywords):
            error_msg = f"""âŒ **LLMæœåŠ¡è¿æ¥å¤±è´¥**

**å¯èƒ½åŸå› **ï¼š
1. ğŸ”´ OllamaæœåŠ¡æœªå¯åŠ¨
2. âš ï¸ Ollamaåœ°å€é…ç½®é”™è¯¯
3. ğŸŒ ç½‘ç»œè¿æ¥é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**ï¼š

ğŸ’¡ **æ–¹æ¡ˆ1ï¼šå¯åŠ¨OllamaæœåŠ¡**
```bash
ollama serve
```

ğŸ’¡ **æ–¹æ¡ˆ2ï¼šåˆ‡æ¢åˆ°OpenAIæ¨¡å¼**
åœ¨å·¦ä¾§"ğŸ¤– LLMé…ç½®"åŒºåŸŸï¼š
- é€‰æ‹© `openai` æä¾›å•†
- å¡«å†™API Keyå’Œæ¨¡å‹åç§°
- ç‚¹å‡»"ä¿å­˜LLMé…ç½®"

ğŸ“ **é”™è¯¯è¯¦æƒ…**: {str(e)}"""
        else:
            error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
        
        history[-1]["content"] = error_msg
        yield history, {"error": str(e)}, ""


def handle_clear() -> Tuple[List[dict], dict]:
    """
    æ¸…ç©ºå¯¹è¯

    Returns:
        Tuple[List[dict], dict]: (ç©ºå†å², ç©ºæ¥æº)
    """
    pipeline = get_pipeline()
    pipeline.clear_conversation("gradio_session")
    return [], {}


def handle_delete_doc(doc_id: str) -> Tuple[str, List[List]]:
    """
    åˆ é™¤æ–‡æ¡£

    Args:
        doc_id: æ–‡æ¡£ID

    Returns:
        Tuple[str, List[List]]: (çŠ¶æ€æ¶ˆæ¯, æ›´æ–°åçš„æ–‡æ¡£åˆ—è¡¨)
    """
    if not doc_id:
        return "è¯·é€‰æ‹©è¦åˆ é™¤çš„æ–‡æ¡£", get_document_list()

    pipeline = get_pipeline()
    success = pipeline.delete_document(doc_id)

    if success:
        return f"âœ… æ–‡æ¡£å·²åˆ é™¤", get_document_list()
    else:
        return f"âŒ åˆ é™¤å¤±è´¥", get_document_list()


def handle_clear_all() -> Tuple[str, List[List], List[dict], dict]:
    """
    æ¸…ç©ºæ‰€æœ‰æ–‡æ¡£

    Returns:
        Tuple: (çŠ¶æ€æ¶ˆæ¯, ç©ºæ–‡æ¡£åˆ—è¡¨, ç©ºå¯¹è¯å†å², ç©ºæ¥æº)
    """
    pipeline = get_pipeline()
    pipeline.clear_all_data()
    pipeline.clear_conversation("gradio_session")
    return "âœ… å·²æ¸…ç©ºæ‰€æœ‰æ–‡æ¡£", [], [], {}


def get_current_llm_config() -> Dict:
    """
    è·å–å½“å‰LLMé…ç½®
    
    Returns:
        Dict: åŒ…å«provider, ollama_model, ollama_url, openai_model, openai_key, openai_url
    """
    config = get_config()
    return {
        "provider": config.LLM_PROVIDER,
        "ollama_model": config.LLM_MODEL if config.LLM_PROVIDER == "ollama" else "qwen2.5:7b",
        "ollama_url": config.LLM_BASE_URL,
        "openai_model": config.OPENAI_MODEL,
        "openai_key": config.OPENAI_API_KEY,
        "openai_url": config.LLM_BASE_URL if config.LLM_PROVIDER == "openai" else "https://api.openai.com/v1"
    }


def handle_llm_config_update(
    provider: str,
    ollama_model: str,
    ollama_url: str,
    openai_model: str,
    openai_key: str,
    openai_url: str
) -> str:
    """
    æ›´æ–°LLMé…ç½®
    
    Args:
        provider: LLMæä¾›å•†
        ollama_model: Ollamaæ¨¡å‹åç§°
        ollama_url: Ollama APIåœ°å€
        openai_model: OpenAIæ¨¡å‹åç§°
        openai_key: OpenAI APIå¯†é’¥
        openai_url: OpenAI APIåœ°å€
        
    Returns:
        str: çŠ¶æ€æ¶ˆæ¯
    """
    pipeline = get_pipeline()
    pipeline.initialize()
    
    try:
        if provider == "ollama":
            success = pipeline.update_generator(
                provider=provider,
                model=ollama_model,
                base_url=ollama_url
            )
        else:  # openai
            if not openai_key:
                return "âŒ è¯·è¾“å…¥OpenAI API Key"
            success = pipeline.update_generator(
                provider=provider,
                model=openai_model,
                base_url=openai_url,
                api_key=openai_key
            )
        
        if success:
            return f"âœ… LLMé…ç½®å·²æ›´æ–°: {provider} / {ollama_model if provider == 'ollama' else openai_model}"
        else:
            return "âŒ é…ç½®æ›´æ–°å¤±è´¥"
    except Exception as e:
        return f"âŒ é…ç½®æ›´æ–°å¤±è´¥: {str(e)}"


def handle_provider_change(provider: str):
    """
    å¤„ç†Provideråˆ‡æ¢
    
    Args:
        provider: é€‰æ‹©çš„Provider
        
    Returns:
        Tuple: æ§åˆ¶å„é…ç½®åŒºåŸŸçš„å¯è§æ€§
    """
    if provider == "ollama":
        return gr.update(visible=True), gr.update(visible=False)
    else:
        return gr.update(visible=False), gr.update(visible=True)


# ==================== åˆ›å»ºåº”ç”¨ ====================

def create_app() -> gr.Blocks:
    """
    åˆ›å»ºGradioåº”ç”¨

    Returns:
        gr.Blocks: Gradioåº”ç”¨å®ä¾‹
    """
    with gr.Blocks(
        title="RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ"
    ) as app:

        # æ ‡é¢˜
        gr.Markdown("""
        # ğŸ¤– RAGå¢å¼ºæ™ºèƒ½é—®ç­”ç³»ç»Ÿ
        
        åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯çš„ä¸­è‹±åŒè¯­æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€‚ä¸Šä¼ æ–‡æ¡£ï¼Œç„¶ååŸºäºæ–‡æ¡£å†…å®¹è¿›è¡Œé—®ç­”ã€‚
        
        **ç‰¹è‰²åŠŸèƒ½**: æ··åˆæ£€ç´¢ | æ™ºèƒ½åˆ†å— | ç­”æ¡ˆæº¯æº | ç»“æœé‡æ’åº | å¤šè½®å¯¹è¯
        
        ğŸ’¡ ç‚¹å‡»å·¦ä¾§ "ğŸ¯ å¿«é€Ÿå¼€å§‹ï¼šåŠ è½½ç¤ºä¾‹æ–‡æ¡£" æŒ‰é’®ç«‹å³ä½“éªŒï¼
        """)

        with gr.Row():
            # ==================== å·¦ä¾§é¢æ¿ ====================
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“ æ–‡æ¡£ç®¡ç†")
                
                # å¿«é€Ÿå¼€å§‹ï¼šç¤ºä¾‹æ–‡æ¡£æŒ‰é’®
                with gr.Row():
                    load_example_btn = gr.Button(
                        "ğŸ¯ å¿«é€Ÿå¼€å§‹ï¼šåŠ è½½ç¤ºä¾‹æ–‡æ¡£", 
                        variant="primary",
                        size="sm"
                    )

                # æ–‡ä»¶ä¸Šä¼ 
                file_upload = gr.File(
                    label="ä¸Šä¼ æ–‡æ¡£",
                    file_types=[".pdf", ".txt", ".docx", ".md"],
                    file_count="multiple"
                )

                upload_btn = gr.Button("ğŸ“¤ ä¸Šä¼ å¹¶ç´¢å¼•", variant="secondary")
                upload_status = gr.Textbox(
                    label="ä¸Šä¼ çŠ¶æ€",
                    interactive=False,
                    lines=6
                )

                gr.Markdown("### ğŸ“‹ å·²ç´¢å¼•æ–‡æ¡£")

                doc_table = gr.Dataframe(
                    headers=["æ–‡æ¡£å", "å—æ•°", "ID"],
                    datatype=["str", "number", "str"],
                    label="æ–‡æ¡£åˆ—è¡¨",
                    interactive=False,
                    value=get_document_list
                )

                with gr.Row():
                    refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°", size="sm")
                    clear_all_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå…¨éƒ¨", size="sm", variant="stop")

                # åˆ é™¤åŠŸèƒ½
                with gr.Row():
                    delete_id = gr.Textbox(
                        label="è¦åˆ é™¤çš„æ–‡æ¡£ID",
                        placeholder="ä»ä¸Šè¡¨å¤åˆ¶ID",
                        scale=2
                    )
                    delete_btn = gr.Button("åˆ é™¤", size="sm", scale=1)

                gr.Markdown("### âš™ï¸ æ£€ç´¢è®¾ç½®")

                top_k_slider = gr.Slider(
                    minimum=1,
                    maximum=10,
                    value=5,
                    step=1,
                    label="æ£€ç´¢æ•°é‡ (Top-K)"
                )

                stream_mode = gr.Checkbox(
                    label="æµå¼è¾“å‡º",
                    value=True
                )
                
                gr.Markdown("### ğŸ¤– LLM é…ç½®")
                
                # è·å–å½“å‰é…ç½®
                current_config = get_config()
                
                llm_provider = gr.Dropdown(
                    choices=["ollama", "openai"],
                    value=current_config.LLM_PROVIDER,
                    label="LLM æä¾›å•†",
                    info="é€‰æ‹©ä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹æˆ–OpenAI API"
                )
                
                # Ollamaé…ç½®åŒº
                with gr.Group(visible=(current_config.LLM_PROVIDER == "ollama")) as ollama_config:
                    ollama_model = gr.Textbox(
                        label="Ollama æ¨¡å‹",
                        value=current_config.LLM_MODEL,
                        placeholder="ä¾‹å¦‚: qwen2.5:7b, llama3:8b"
                    )
                    ollama_url = gr.Textbox(
                        label="Ollama åœ°å€",
                        value=current_config.LLM_BASE_URL,
                        placeholder="http://localhost:11434"
                    )
                
                # OpenAIé…ç½®åŒº
                with gr.Group(visible=(current_config.LLM_PROVIDER == "openai")) as openai_config:
                    openai_url = gr.Textbox(
                        label="API Base URL",
                        value="https://api.openai.com/v1",
                        placeholder="https://api.openai.com/v1 æˆ–è‡ªå®šä¹‰åœ°å€"
                    )
                    openai_key = gr.Textbox(
                        label="API Key",
                        value=current_config.OPENAI_API_KEY,
                        placeholder="sk-...",
                        type="password"
                    )
                    openai_model = gr.Textbox(
                        label="æ¨¡å‹åç§°",
                        value=current_config.OPENAI_MODEL,
                        placeholder="ä¾‹å¦‚: gpt-3.5-turbo, gpt-4"
                    )
                
                llm_save_btn = gr.Button("ğŸ’¾ ä¿å­˜LLMé…ç½®", variant="secondary", size="sm")
                llm_status = gr.Textbox(
                    label="é…ç½®çŠ¶æ€",
                    interactive=False,
                    lines=1
                )

            # ==================== å³ä¾§é¢æ¿ ====================
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ’¬ é—®ç­”å¯¹è¯")

                chatbot = gr.Chatbot(
                    label="å¯¹è¯å†å²",
                    height=450,
                    show_label=False
                )

                with gr.Row():
                    question_input = gr.Textbox(
                        label="è¾“å…¥é—®é¢˜",
                        placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ŒæŒ‰å›è½¦å‘é€...",
                        lines=1,
                        scale=5
                    )
                    send_btn = gr.Button("ğŸš€ å‘é€", variant="primary", scale=1)

                clear_chat_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", size="sm")

                gr.Markdown("### ğŸ“š æ¥æºå¼•ç”¨")

                sources_json = gr.JSON(
                    label="ç­”æ¡ˆæ¥æº"
                )

        # ==================== é¡µè„š ====================
        gr.Markdown("""
        ---
        
        <div style="text-align: center; color: #666;">
        
        **æŠ€æœ¯æ ˆ**: BGE-M3 åµŒå…¥ | ChromaDB å‘é‡åº“ | Qwen2.5/Ollama LLM | FastAPI | Gradio
        
        </div>
        """)

        # ==================== äº‹ä»¶ç»‘å®š ====================

        # åŠ è½½ç¤ºä¾‹æ–‡æ¡£
        load_example_btn.click(
            fn=handle_load_example,
            inputs=[],
            outputs=[upload_status, doc_table, question_input]
        )

        # ä¸Šä¼ äº‹ä»¶
        upload_btn.click(
            fn=handle_upload,
            inputs=[file_upload],
            outputs=[upload_status, doc_table]
        )

        # åˆ·æ–°æ–‡æ¡£åˆ—è¡¨
        refresh_btn.click(
            fn=lambda: get_document_list(),
            inputs=[],
            outputs=[doc_table]
        )

        # åˆ é™¤æ–‡æ¡£
        delete_btn.click(
            fn=handle_delete_doc,
            inputs=[delete_id],
            outputs=[upload_status, doc_table]
        )

        # æ¸…ç©ºæ‰€æœ‰æ–‡æ¡£ï¼ˆåŒæ—¶æ¸…ç©ºå¯¹è¯å’Œæ¥æºï¼‰
        clear_all_btn.click(
            fn=handle_clear_all,
            inputs=[],
            outputs=[upload_status, doc_table, chatbot, sources_json]
        )

        # å‘é€é—®é¢˜ - æ ¹æ®æµå¼æ¨¡å¼é€‰æ‹©å¤„ç†å‡½æ•°
        def query_with_mode(question, history, top_k, use_stream):
            """æ ¹æ®æµå¼æ¨¡å¼é€‰æ‹©å¤„ç†æ–¹å¼"""
            if use_stream:
                # æµå¼æ¨¡å¼ï¼šä½¿ç”¨ yield from ä¼ é€’ç”Ÿæˆå™¨
                yield from handle_query_stream(question, history, top_k)
            else:
                # éæµå¼æ¨¡å¼ï¼šä¹Ÿä½¿ç”¨ç”Ÿæˆå™¨ä»¥æ”¯æŒå³æ—¶æ˜¾ç¤º
                yield from handle_query(question, history, top_k)

        send_btn.click(
            fn=query_with_mode,
            inputs=[question_input, chatbot, top_k_slider, stream_mode],
            outputs=[chatbot, sources_json, question_input]
        )

        # å›è½¦å‘é€
        question_input.submit(
            fn=query_with_mode,
            inputs=[question_input, chatbot, top_k_slider, stream_mode],
            outputs=[chatbot, sources_json, question_input]
        )

        # æ¸…ç©ºå¯¹è¯
        clear_chat_btn.click(
            fn=handle_clear,
            inputs=[],
            outputs=[chatbot, sources_json]
        )
        
        # LLM Provideråˆ‡æ¢äº‹ä»¶
        llm_provider.change(
            fn=handle_provider_change,
            inputs=[llm_provider],
            outputs=[ollama_config, openai_config]
        )
        
        # ä¿å­˜LLMé…ç½®äº‹ä»¶
        llm_save_btn.click(
            fn=handle_llm_config_update,
            inputs=[llm_provider, ollama_model, ollama_url, openai_model, openai_key, openai_url],
            outputs=[llm_status]
        )

    return app


def launch_app(
    host: str = None,
    port: int = None,
    share: bool = False
) -> None:
    """
    å¯åŠ¨Gradioåº”ç”¨

    Args:
        host: ä¸»æœºåœ°å€
        port: ç«¯å£å·
        share: æ˜¯å¦åˆ›å»ºå…¬å…±é“¾æ¥
    """
    config = get_config()
    host = host or config.HOST
    port = port or config.PORT

    # é¢„åˆå§‹åŒ–
    logger.info("æ­£åœ¨åˆå§‹åŒ–RAGç³»ç»Ÿ...")
    pipeline = get_pipeline()
    pipeline.initialize()
    logger.info("RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    # åˆ›å»ºå¹¶å¯åŠ¨åº”ç”¨
    app = create_app()
    app.launch(
        server_name=host,
        server_port=port,
        share=share,
        show_error=True
    )


# ==================== ä¸»å…¥å£ ====================

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="ä¸»æœºåœ°å€")
    parser.add_argument("--port", type=int, default=7860, help="ç«¯å£å·")
    parser.add_argument("--share", action="store_true", help="åˆ›å»ºå…¬å…±é“¾æ¥")

    args = parser.parse_args()

    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    launch_app(host=args.host, port=args.port, share=args.share)
